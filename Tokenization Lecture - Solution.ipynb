{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies the lectures on tokenization, normalization, and stemming. \n",
    "\n",
    "Before we get started, you're going to need the NLTK book corpus. Here are the steps to install it: \n",
    "\n",
    "1. Open a console or command window.\n",
    "1. Type `python` to start using python. \n",
    "1. Type `import nltk` and hit enter.\n",
    "1. Type `nltk.download()` and hit enter.\n",
    "1. This will open a little window. \n",
    "1. Click \"All Packages\" at the top of the list. \n",
    "1. Click \"Download\"\n",
    "\n",
    "Let me know if you run into any issues!\n",
    "\n",
    "---\n",
    "\n",
    "Now we can get started in earnest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process by which we split text up into tokens. The simplest tokens are those split by whitespace. Let's begin by counting the words in a file that I've included the in repo: the text of _Beowulf_. \n",
    "\n",
    "1. Read the file into a variable that holds a (large) string. \n",
    "1. Look at the first 1000 characters of that string.\n",
    "1. Split that string on whitespace (spaces, returns, tabs, etc.)\n",
    "1. Count the number of tokens. \n",
    "1. Determine the most common 10 tokens. \n",
    "1. Find the tokens that include punctuation within the token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beowulf = open(\"beowulf.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beowulf[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_tokens = beowulf.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(beo_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to count things. The handiest is the `Counter` data type. I'll illustrate it's use and show you how you could do the same thing with a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the Counter version\n",
    "from collections import Counter # Typically you'd do this at the top of the notebook.\n",
    "\n",
    "beo_counter = Counter(beo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can do things like find the most common tokens\n",
    "beo_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Counter` is really just a dictionary, where the keys are the elements of the list that you fed in, and the values are the integer counts. Here's how you'd do the same thing with a dictionary. Notice how much more difficult it is, and the weird construction to sort the dictionary by values for reading out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_dict = dict()\n",
    "\n",
    "for t in beo_tokens :\n",
    "    \n",
    "    # Have to create the spot in the dictionary if it's not in there.\n",
    "    if t not in beo_dict :\n",
    "        beo_dict[t] = 0\n",
    "    \n",
    "    # And now increment the count\n",
    "    beo_dict[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the top 10 is pretty tricky. Do the work to understand what's happening below here:\n",
    "num_printed = 0\n",
    "\n",
    "for token, count in sorted(beo_dict.items(), key=lambda item: -1*item[1]) :\n",
    "    print(token + \" had \" + str(count) + \" instances.\")\n",
    "    num_printed += 1\n",
    "    \n",
    "    if num_printed == 10 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's count the number of tokens that have punctuation in them. \n",
    "# Python has an object that holds punctuation, so we can use that. \n",
    "from string import punctuation # usually would do this at the top\n",
    "\n",
    "# We'll use a set trick, so need punctuation in a set\n",
    "punct_set = set(punctuation)\n",
    "\n",
    "beo_tokens_punct = []\n",
    "\n",
    "for w in beo_tokens :\n",
    "    w_set = set(w)\n",
    "    overlap = w_set.intersection(punct_set)\n",
    "    \n",
    "    if len(overlap) > 0 :\n",
    "        beo_tokens_punct.append(w)\n",
    "\n",
    "        \n",
    "print(beo_tokens_punct[:100])\n",
    "print(len(beo_tokens_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you look at those tokens with punctuation, what do you notice? \n",
    "\n",
    "1. Lots of commas just stuck onto words.\n",
    "1. Some tokens that are *just* punctuation (e.g., \"--\").\n",
    "1. Lots of capitals that might not be what we want in our tokenization. \n",
    "1. A lot of tokens with punctuation. The next cell calculates the fraction. (Note that it's not unique counts yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(beo_tokens_punct)/len(beo_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Second Exercise\n",
    "\n",
    "Now let's try working with some NLTK data. Count the words (or, more precisely, tokens) in one of the first three books included in the book corpus (_Moby Dick_, _Sense and Sensibility_, and The Book of Genesis from the _Bible_).\n",
    "\n",
    "1. Pick one of the texts and assign it to a new variable. It'll have a name like `text1` before you assign it. That variable was created when we imported everything from `nltk.book`.\n",
    "1. Look at the structure of variable.\n",
    "1. Count the tokens as above. Use the `Counter` object.\n",
    "1. Display the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense = text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(sense).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pull in 10 copora when we load the books, but there are a *ton* of books we get with NLTK. Here are the ones that come with Project Gutenberg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in nltk.corpus.gutenberg.fileids() :\n",
    "    print(book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
